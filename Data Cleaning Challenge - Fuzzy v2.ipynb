{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinkful Challenge: Data Cleaning & Validation\n",
    "\n",
    "#### In this challenge, I will use this dataset of article open-access prices paid by the WELLCOME Trust between 2012 and 2013. To complete this challenge, I will determine the five most common journals and the total articles for each. I will also calculate the mean, median, and standard deviation of the open-access cost for each journal. The data will have to be cleaned thoroughly in order to extract accurate estimates, and I will describe the steps I have taken below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll import all of the packages we need. Then we can take a look at what we're working with by reading in the CSV and printing out a few lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics as stats\n",
    "\n",
    "import fuzzywuzzy\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PMID</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Journal</th>\n",
       "      <th>Article</th>\n",
       "      <th>Cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>CUP</td>\n",
       "      <td>Psychological Medicine</td>\n",
       "      <td>Reduced parahippocampal cortical thickness in ...</td>\n",
       "      <td>£0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PMC3679557</td>\n",
       "      <td>ACS</td>\n",
       "      <td>Biomacromolecules</td>\n",
       "      <td>Structural characterization of a Model Gram-ne...</td>\n",
       "      <td>£2381.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23043264  PMC3506128</td>\n",
       "      <td>ACS</td>\n",
       "      <td>J Med Chem</td>\n",
       "      <td>Fumaroylamino-4,5-epoxymorphinans and related ...</td>\n",
       "      <td>£642.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23438330 PMC3646402</td>\n",
       "      <td>ACS</td>\n",
       "      <td>J Med Chem</td>\n",
       "      <td>Orvinols with mixed kappa/mu opioid receptor a...</td>\n",
       "      <td>£669.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23438216 PMC3601604</td>\n",
       "      <td>ACS</td>\n",
       "      <td>J Org Chem</td>\n",
       "      <td>Regioselective opening of myo-inositol orthoes...</td>\n",
       "      <td>£685.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PMC3579457</td>\n",
       "      <td>ACS</td>\n",
       "      <td>Journal of Medicinal Chemistry</td>\n",
       "      <td>Comparative Structural and Functional Studies ...</td>\n",
       "      <td>£2392.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>PMC3709265</td>\n",
       "      <td>ACS</td>\n",
       "      <td>Journal of Proteome Research</td>\n",
       "      <td>Mapping Proteolytic Processing in the Secretom...</td>\n",
       "      <td>£2367.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>23057412 PMC3495574</td>\n",
       "      <td>ACS</td>\n",
       "      <td>Mol Pharm</td>\n",
       "      <td>Quantitative silencing of EGFP reporter gene b...</td>\n",
       "      <td>£649.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PMCID: PMC3780468</td>\n",
       "      <td>ACS (Amercian Chemical Society) Publications</td>\n",
       "      <td>ACS Chemical Biology</td>\n",
       "      <td>A Novel Allosteric Inhibitor of the Uridine Di...</td>\n",
       "      <td>£1294.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PMCID: PMC3621575</td>\n",
       "      <td>ACS (Amercian Chemical Society) Publications</td>\n",
       "      <td>ACS Chemical Biology</td>\n",
       "      <td>Chemical proteomic analysis reveals the drugab...</td>\n",
       "      <td>£1294.78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    PMID                                     Publisher  \\\n",
       "0                    NaN                                           CUP   \n",
       "1             PMC3679557                                           ACS   \n",
       "2  23043264  PMC3506128                                            ACS   \n",
       "3    23438330 PMC3646402                                           ACS   \n",
       "4   23438216 PMC3601604                                            ACS   \n",
       "5             PMC3579457                                           ACS   \n",
       "6             PMC3709265                                           ACS   \n",
       "7   23057412 PMC3495574                                            ACS   \n",
       "8      PMCID: PMC3780468  ACS (Amercian Chemical Society) Publications   \n",
       "9      PMCID: PMC3621575  ACS (Amercian Chemical Society) Publications   \n",
       "\n",
       "                          Journal  \\\n",
       "0          Psychological Medicine   \n",
       "1               Biomacromolecules   \n",
       "2                      J Med Chem   \n",
       "3                      J Med Chem   \n",
       "4                      J Org Chem   \n",
       "5  Journal of Medicinal Chemistry   \n",
       "6    Journal of Proteome Research   \n",
       "7                       Mol Pharm   \n",
       "8            ACS Chemical Biology   \n",
       "9            ACS Chemical Biology   \n",
       "\n",
       "                                             Article      Cost  \n",
       "0  Reduced parahippocampal cortical thickness in ...     £0.00  \n",
       "1  Structural characterization of a Model Gram-ne...  £2381.04  \n",
       "2  Fumaroylamino-4,5-epoxymorphinans and related ...   £642.56  \n",
       "3  Orvinols with mixed kappa/mu opioid receptor a...   £669.64  \n",
       "4  Regioselective opening of myo-inositol orthoes...   £685.88  \n",
       "5  Comparative Structural and Functional Studies ...  £2392.20  \n",
       "6  Mapping Proteolytic Processing in the Secretom...  £2367.95  \n",
       "7  Quantitative silencing of EGFP reporter gene b...   £649.33  \n",
       "8  A Novel Allosteric Inhibitor of the Uridine Di...  £1294.59  \n",
       "9  Chemical proteomic analysis reveals the drugab...  £1294.78  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df = pd.read_csv('WELLCOME_APCspend2013_forThinkful.csv', encoding=\"latin-1\")         # Read in the CSV file\n",
    "\n",
    "main_df.columns = ['PMID', 'Publisher', 'Journal', 'Article', \"Cost\"]                      # Rename some columns for ease\n",
    "\n",
    "main_df.head(10)                                                                           # Display the first 10 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are aiming to use this data set to find statistics related to the journals, so if the journal title is not included at all, the other pieces of data are not useful for our specific research. Let's drop anything matching that criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_df = main_df.dropna(subset=['Journal'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll define a function that will get rid of all the pesky empty space that could be lurking inside the many strings of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_all_columns(data_frame_in):\n",
    "    \n",
    "    # Trim whitespace from ends of each value across all series in dataframe, given that it's a string\n",
    "    trim_strings = lambda x: x.strip() if type(x) is str else x\n",
    "    return data_frame_in.applymap(trim_strings)\n",
    "\n",
    "\n",
    "main_df = trim_all_columns(main_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also noticed there are some cost values that don't make sense, such as £999999.00 or a number with an American dollar sign. The documentation says all these costs were already converted to £ sterling so we will assume we can simply drop all signs and any amounts that are clearly inaccurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So let's take out all the currency symbols\n",
    "main_df['Cost'] = main_df['Cost'].map(lambda x: x.strip('£'))\n",
    "\n",
    "main_df['Cost'] = main_df['Cost'].map(lambda y: y.strip('$'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can easily get rid of the costs that are equal to '999999.00', of which there are multiple. We will replace them with the mean value of the data set as it's likely they weren't meant to be = 0, but we also don't want to get rid of them altogether."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So first, convert the cost column into numbers, and sort them in descending order\n",
    "main_df['Cost'] = pd.to_numeric(main_df.Cost)\n",
    "main_df = main_df.sort_values(by=['Cost'], ascending=False)\n",
    "\n",
    "# Now calculate the mean of the values that are not 999999 (there are 47 of these cases)\n",
    "cost_avg = round(main_df['Cost'][47:].mean(), 2)\n",
    "\n",
    "# Then replace all of the incorrect 999999 costs with the mean\n",
    "main_df['Cost'].replace(999999.0, cost_avg, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've cleaned up the cost column, it's time to clean up the other column that we'll be using to calculate the specific statistics that are being asked for. Thus, we will begin cleaning the \"Journal Title\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Start off by replacing characters we don't necessarily need, such as colons, periods, parenthesis, dashes, etc\n",
    "main_df['Journal'] = main_df['Journal'].str.replace(':', '')\n",
    "\n",
    "main_df['Journal'] = main_df['Journal'].str.replace(' - ', ' ')\n",
    "main_df['Journal'] = main_df['Journal'].str.replace('--', '-')\n",
    "main_df['Journal'] = main_df['Journal'].str.replace('-', ' ')\n",
    "\n",
    "main_df['Journal'] = main_df['Journal'].str.replace(\"'\", \"\")\n",
    "\n",
    "main_df['Journal'] = main_df['Journal'].str.replace('.', '')\n",
    "\n",
    "main_df['Journal'] = main_df['Journal'].str.replace('(', '')\n",
    "main_df['Journal'] = main_df['Journal'].str.replace(')', '')\n",
    "\n",
    "main_df['Journal'] = main_df['Journal'].str.replace(',', '')\n",
    "\n",
    "main_df['Journal'] = main_df['Journal'].str.replace('   ', ' ')\n",
    "main_df['Journal'] = main_df['Journal'].str.replace('  ', ' ')\n",
    "\n",
    "# I'm also going to replace ampersands (&) with the actual word \"and\"\n",
    "main_df['Journal'] = main_df['Journal'].str.replace('&', 'and')\n",
    "\n",
    "# Then I'll sort everything by the Journal name to help me look through it\n",
    "main_df = main_df.sort_values(by=['Journal'], ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following this we will capitalize everything to make it easier to clean the data. This way if two things are spelled exactly the same but have different cases they will now be equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df['Journal'] = main_df['Journal'].str.upper() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'm going to use the fuzzywuzzy package to standardize the names of journals via more replacement. I do this by making a list of unique values and iterating through them to fix the dataframe. This way, by the end of the loop it should have packaged each unique journal into one name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to replace rows in the provided column of the provided dataframe\n",
    "# that match the provided string above the provided ratio with the provided string\n",
    "def replace_matches_in_column(df, column, string_to_match, min_ratio = 90):\n",
    "    # get a list of unique strings\n",
    "    strings = df[column].unique()\n",
    "    \n",
    "    # get the top 10 closest matches to our input string\n",
    "    matches = fuzzywuzzy.process.extract(string_to_match, strings, \n",
    "                                         limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n",
    "\n",
    "    # only get matches with a ratio > 90\n",
    "    close_matches = [matches[0] for matches in matches if matches[1] >= min_ratio]\n",
    "\n",
    "    # get the rows of all the close matches in our dataframe\n",
    "    rows_with_matches = df[column].isin(close_matches)\n",
    "\n",
    "    # replace all rows with close matches with the input matches \n",
    "    df.loc[rows_with_matches, column] = string_to_match\n",
    "\n",
    "\n",
    "all_strings = main_df['Journal'].unique()\n",
    "\n",
    "for each_name in all_strings:\n",
    "    replace_matches_in_column(df=main_df, column='Journal', string_to_match=each_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready create our descriptive statistics. First we'll drop the columns that we won't need for right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = main_df.drop(columns=\"PMID\")\n",
    "main_df = main_df.drop(columns=\"Publisher\")\n",
    "main_df = main_df.drop(columns=\"Article\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following this, we can show the number of times each journal comes up and display this number. We will show the top 5 journals for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Journal Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PLOS ONE</th>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JOURNAL BIOLOGICAL CHEMISTRY</th>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NEUROLMAGE</th>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUCLEIC ACID RESEARCH</th>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES PNAS</th>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Journal Frequency\n",
       "PLOS ONE                                                          191\n",
       "JOURNAL BIOLOGICAL CHEMISTRY                                       61\n",
       "NEUROLMAGE                                                         31\n",
       "NUCLEIC ACID RESEARCH                                              29\n",
       "PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES...                 25"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count it\n",
    "count_df = main_df['Journal'].value_counts()\n",
    "\n",
    "# Put it back into a dataframe\n",
    "count_df = count_df.to_frame()\n",
    "\n",
    "# Change the column name for ease of reading\n",
    "count_df.columns = ['Journal Frequency']\n",
    "\n",
    "# Show it\n",
    "count_df.head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all that, we can now show the other summary statistics we listed above. First I will calculate the sum of all costs attributed to each journal and display the top 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cost</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Journal</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PLOS ONE</th>\n",
       "      <td>377670.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MOVEMENT DISORDERS</th>\n",
       "      <td>227651.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JOURNAL BIOLOGICAL CHEMISTRY</th>\n",
       "      <td>86485.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NEUROLMAGE</th>\n",
       "      <td>68577.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NATURE COMMUNICATIONS</th>\n",
       "      <td>58424.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DEVELOPMENTAL CELL</th>\n",
       "      <td>43137.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PLOS GENETICS</th>\n",
       "      <td>40167.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PLOS NEGECTED TROPICAL DISEASES</th>\n",
       "      <td>40041.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HUMAN MOLECULAR GENETICS</th>\n",
       "      <td>38748.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CURRENT BIOLOGY</th>\n",
       "      <td>38628.51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Cost\n",
       "Journal                                   \n",
       "PLOS ONE                         377670.48\n",
       "MOVEMENT DISORDERS               227651.82\n",
       "JOURNAL BIOLOGICAL CHEMISTRY      86485.16\n",
       "NEUROLMAGE                        68577.62\n",
       "NATURE COMMUNICATIONS             58424.06\n",
       "DEVELOPMENTAL CELL                43137.90\n",
       "PLOS GENETICS                     40167.76\n",
       "PLOS NEGECTED TROPICAL DISEASES   40041.25\n",
       "HUMAN MOLECULAR GENETICS          38748.41\n",
       "CURRENT BIOLOGY                   38628.51"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group our data by the journal name and sum their total costs\n",
    "sum_df = main_df.groupby('Journal')['Cost'].sum()\n",
    "\n",
    "# Change this output back into a dataframe so that we can show it easily and print out the highest total costs\n",
    "sum_df = sum_df.to_frame()\n",
    "\n",
    "sum_df = sum_df.sort_values(by=['Cost'], ascending=False)\n",
    "\n",
    "sum_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can calculate an accurate mean, median, and standard deviation for each journal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean of all costs is: 2009.66\n",
      "The median of all costs is: 1883.86\n",
      "The standard deviation of all costs is: 6029.15\n"
     ]
    }
   ],
   "source": [
    "full_df_mean = round(main_df['Cost'].mean(), 2)\n",
    "print(\"The mean of all costs is: \" + str(full_df_mean))\n",
    "\n",
    "full_df_med = round(main_df['Cost'].median(), 2)\n",
    "print(\"The median of all costs is: \" + str(full_df_med))\n",
    "\n",
    "full_df_sdv = round(main_df['Cost'].std(), 2)\n",
    "print(\"The standard deviation of all costs is: \" + str(full_df_sdv))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
